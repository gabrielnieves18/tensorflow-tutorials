{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call %matplotlib only in a Jupiter notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# Common and local imports\n",
    "import dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* **OpenCV:** We use openCV to read images of cats/Dogs so you will have to install it. <br /><br />\n",
    "\n",
    "* **Shape function:** If you have multi-dimensional Tensor in TF, you can get the shape of it by doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 16 128 128   3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.truncated_normal([16,128,128,3])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(tf.shape(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   16 49152]\n"
     ]
    }
   ],
   "source": [
    "# Reshape \"a\" into 2D Tensor with shape Array([16, 49152])\n",
    "with tf.Session() as sess:\n",
    "    b=tf.reshape(a, [16,49152])\n",
    "    print(sess.run(tf.shape(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Note:*** We have collapse the last 3 dimentions into 1 single dimention. \n",
    "\n",
    "> $$ 49152 = (128\\times 128\\times 3) $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAEyCAYAAABdxWyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHP9JREFUeJzt3X+QnVd93/H3t3b8Ay+VbBQ2jqSJ\nTKOSGquZ2lvbCZnMLk5AthnktsDYdUEiZlQam5JGGTBhUs8QaEQSw0AgdFSs2m5cL44DserINcaw\n42GmMraM8foHxIsRoB0hYWxEBAYi8u0f94hc1ivt1d69597d5/2a2dnnnufsfc5Xz+7qs+f5FZmJ\nJEmS6vkn/R6AJElS0xjAJEmSKjOASZIkVWYAkyRJqswAJkmSVJkBTJIkqTIDmCRJUmUGMEmSpMoM\nYJIkSZWd2O8BHMuKFStyzZo1Pd/O9773PU477bSeb2cQNbl2aHb91t7M2qHZ9Te5dmh2/TVq3717\n99OZ+bOd9B3oALZmzRoefPDBnm9nYmKC0dHRnm9nEDW5dmh2/dY+2u9h9E2T629y7dDs+mvUHhFf\n67SvhyAlSZIqM4BJkiRVZgCTJEmqzAAmSZJUmQFMkiSpMgOYJElSZQYwSZKkygxgkiRJlRnAJEmS\nKjOASZIkVWYAkyRJqmygnwUpSf225tq/6ajfnq2X9ngkkpYSZ8AkSZIqcwZMUuNMTh9kU4czW5LU\nC86ASZIkVWYAkyRJqswAJkmSVJkBTJIkqTIDmCRJUmUGMEmSpMoMYJIkSZUZwCRJkiozgEmSJFXm\nnfAl9Y3PWZTUVHPOgEXE9og4EBGPzmh/a0R8KSIei4g/bmt/Z0RMRcSXI+JVbe3rS9tURFy7sGVI\nkiQtHp3MgN0IfBi4+UhDRIwBG4BfzswfRsSLS/vZwOXAy4CfBz4dEf+8fNlHgN8E9gIPRMSOzHx8\noQqRJElaLOYMYJl5X0SsmdH8n4CtmfnD0udAad8AjJf2r0bEFHB+WTeVmU8BRMR46WsAk7QkeDhV\n0vGIzJy7UyuA3ZmZ55TXDwN3AOuBHwC/l5kPRMSHgV2Z+Rel3w3AXeVt1mfmm0v7G4ALMvOaWba1\nGdgMMDw8fN74+HhXBXbi0KFDDA0N9Xw7g6jJtUOz6x+E2ienD3bUb93KZQu63QPPHGT/cwv6lgtu\noWtuNwj7vl+aXDs0u/4atY+Nje3OzJFO+s73JPwTgTOAC4F/DdwWES+Z53v9lMzcBmwDGBkZydHR\n0YV422OamJigxnYGUZNrh2bX38vaO50N6vRX0J4rR+c9ltn82S13cP3kYF+DtNA1t/P7frTfw+ib\nJtc/aLXP9zfQXuAT2Zo++3xE/AOwApgGVrf1W1XaOEa7JB2Th/ckLTXzDWB/DYwBny0n2Z8EPA3s\nAP53RLyf1kn4a4HPAwGsjYizaAWvy4F/3+XYJemndBrUtqzr8UAkaQ5zBrCIuBUYBVZExF7gOmA7\nsL3cmuJHwMYyG/ZYRNxG6+T6w8DVmfnj8j7XAHcDJwDbM/OxHtQjSZI08Dq5CvKKo6z6D0fp/17g\nvbO07wR2HtfoJEmSliAfRSRJklSZAUySJKkyA5gkSVJlBjBJkqTKDGCSJEmVGcAkSZIqG+xncUga\nKJ0/YkiSdCwGMEkGK0mqzEOQkiRJlRnAJEmSKvMQpCQNoIU+LLxn66UL+n6SuuMMmCRJUmUGMEmS\npMoMYJIkSZV5DpgkNUD7OWVb1h1m01HOMfNcMakOZ8AkSZIqM4BJkiRVZgCTJEmqzAAmSZJUmQFM\nkiSpMgOYJElSZXMGsIjYHhEHIuLRWdZtiYiMiBXldUTEhyJiKiIeiYhz2/pujIgny8fGhS1DkiRp\n8ehkBuxGYP3MxohYDbwS+Hpb88XA2vKxGfho6XsGcB1wAXA+cF1EnN7NwCVJkharOQNYZt4HPDPL\nqg8AbweyrW0DcHO27AKWR8SZwKuAezLzmcx8FriHWUKdJElSE0Rmzt0pYg1wZ2aeU15vAF6RmW+L\niD3ASGY+HRF3Alsz83Ol373AO4BR4JTMfE9p/wPgucz801m2tZnW7BnDw8PnjY+Pd1vjnA4dOsTQ\n0FDPtzOImlw7NLv+9tonpw/2eTR1DZ8K+5/r9yj651j1r1u5rO5gKmvyzzw0u/4atY+Nje3OzJFO\n+h73o4gi4gXA79M6/LjgMnMbsA1gZGQkR0dHe7GZnzIxMUGN7QyiJtcOza6/vfajPZZmqdqy7jDX\nTzb3SWzHqn/PlaN1B1NZk3/modn1D1rt8/kN9M+As4AvRgTAKuChiDgfmAZWt/VdVdqmac2CtbdP\nzGPbkjq0Zo5QdaznAUqSeuu4A1hmTgIvPvJ6xiHIHcA1ETFO64T7g5m5LyLuBv5b24n3rwTe2fXo\npQaaK1hJ3Tie7y8f3C3NXye3obgV+H/ASyNib0RcdYzuO4GngCngfwC/DZCZzwB/CDxQPt5d2iRJ\nkhpnzhmwzLxijvVr2pYTuPoo/bYD249zfJIkSUuOd8KXJEmqrLmXAUmSutLp+WKeKyY9nzNgkiRJ\nlRnAJEmSKjOASZIkVWYAkyRJqswAJkmSVJkBTJIkqTIDmCRJUmUGMEmSpMq8Eas0IHzItiQ1hzNg\nkiRJlRnAJEmSKjOASZIkVeY5YFKPeW6XJGkmZ8AkSZIqM4BJkiRV5iFISVJPdXoYfs/WS3s8Emlw\nOAMmSZJUmQFMkiSpMgOYJElSZXMGsIjYHhEHIuLRtrY/iYgvRcQjEfHJiFjetu6dETEVEV+OiFe1\nta8vbVMRce3ClyJJkrQ4dDIDdiOwfkbbPcA5mfkvgb8F3gkQEWcDlwMvK1/z5xFxQkScAHwEuBg4\nG7ii9JUkSWqcOQNYZt4HPDOj7VOZebi83AWsKssbgPHM/GFmfhWYAs4vH1OZ+VRm/ggYL30lSZIa\nZyHOAfst4K6yvBL4Rtu6vaXtaO2SJEmNE5k5d6eINcCdmXnOjPZ3ASPAv83MjIgPA7sy8y/K+hv4\nx3C2PjPfXNrfAFyQmdfMsq3NwGaA4eHh88bHx+dZWucOHTrE0NBQz7cziJpcO9Spf3L6YE/ff76G\nT4X9z/V7FP3R5NphcOtft3JZz7fh77zm1l+j9rGxsd2ZOdJJ33nfiDUiNgGvBi7Kf0xx08Dqtm6r\nShvHaP8pmbkN2AYwMjKSo6Oj8x1ixyYmJqixnUHU5NqhTv2bBvRZkFvWHeb6yWbei7nJtcPg1r/n\nytGeb8Pfec2tf9Bqn9chyIhYD7wdeE1mfr9t1Q7g8og4OSLOAtYCnwceANZGxFkRcRKtE/V3dDd0\nSZKkxWnOP4Ei4lZgFFgREXuB62hd9XgycE9EQOuw41sy87GIuA14HDgMXJ2ZPy7vcw1wN3ACsD0z\nH+tBPZIkSQNvzgCWmVfM0nzDMfq/F3jvLO07gZ3HNTpJkqQlyDvhS5IkVTZ4Z2FKi8SaAT25Xlqs\nOv2Z2rP10h6PROo9Z8AkSZIqM4BJkiRVZgCTJEmqzAAmSZJUmQFMkiSpMgOYJElSZQYwSZKkygxg\nkiRJlRnAJEmSKvNO+NIM3uFektRrzoBJkiRVZgCTJEmqzAAmSZJUmQFMkiSpMgOYJElSZV4FKUla\nVDq9UnnP1kt7PBJp/pwBkyRJqswAJkmSVJkBTJIkqTIDmCRJUmVzBrCI2B4RByLi0ba2MyLinoh4\nsnw+vbRHRHwoIqYi4pGIOLftazaW/k9GxMbelCNJkjT4OpkBuxFYP6PtWuDezFwL3FteA1wMrC0f\nm4GPQiuwAdcBFwDnA9cdCW2SJElNM2cAy8z7gGdmNG8AbirLNwGXtbXfnC27gOURcSbwKuCezHwm\nM58F7uH5oU6SJKkRIjPn7hSxBrgzM88pr7+TmcvLcgDPZubyiLgT2JqZnyvr7gXeAYwCp2Tme0r7\nHwDPZeafzrKtzbRmzxgeHj5vfHy82xrndOjQIYaGhnq+nUHU5Nph9vonpw/2aTR1DZ8K+5/r9yj6\no8m1Q3PqX7dy2fPa/J3X3Ppr1D42NrY7M0c66dv1jVgzMyNi7hTX+fttA7YBjIyM5Ojo6EK99VFN\nTExQYzuDqMm1w+z1b+rwJo+L3ZZ1h7l+spn3Ym5y7dCc+vdcOfq8Nn/nNbf+Qat9vj+B+yPizMzc\nVw4xHijt08Dqtn6rSts0rVmw9vaJeW5bmpfZ7p69Zd3hxgQuSdLgmO9tKHYAR65k3Ajc0db+xnI1\n5IXAwczcB9wNvDIiTi8n37+ytEmSJDXOnDNgEXErrdmrFRGxl9bVjFuB2yLiKuBrwOtL953AJcAU\n8H3gTQCZ+UxE/CHwQOn37syceWK/JElSI8wZwDLziqOsumiWvglcfZT32Q5sP67RSZIkLUHeCV+S\nJKkyA5gkSVJlS/86ZElSI3V65fOerZfWGpL0E86ASZIkVeYMmBa12f7ClSRp0DkDJkmSVJkBTJIk\nqTIDmCRJUmUGMEmSpMoMYJIkSZUZwCRJkiozgEmSJFVmAJMkSarMG7FKkhqt0xs6+8giLSRnwCRJ\nkiozgEmSJFVmAJMkSarMACZJklSZAUySJKkyA5gkSVJl3oZCA6nTy8IlSVqMupoBi4j/EhGPRcSj\nEXFrRJwSEWdFxP0RMRURH4+Ik0rfk8vrqbJ+zUIUIEmStNjMO4BFxErgPwMjmXkOcAJwOfA+4AOZ\n+YvAs8BV5UuuAp4t7R8o/SRJkhqn23PATgROjYgTgRcA+4BXALeX9TcBl5XlDeU1Zf1FERFdbl+S\nJGnRicyc/xdHvA14L/Ac8CngbcCuMstFRKwG7srMcyLiUWB9Zu4t674CXJCZT894z83AZoDh4eHz\nxsfH5z2+Th06dIihoaGeb2cQDWrtk9MHq2xn+FTY/1yVTQ0ca+/3KPqnyfV3U/u6lcsWdjB9MKi/\n82uoUfvY2NjuzBzppO+8T8KPiNNpzWqdBXwH+Etg/Xzf74jM3AZsAxgZGcnR0dFu33JOExMT1NjO\nIBrU2jdVOgl/y7rDXD/ZzGtRrL2ZtUOz6++m9j1Xji7sYPpgUH/n1zBotXdzCPI3gK9m5rcy8++B\nTwAvB5aXQ5IAq4DpsjwNrAYo65cB3+5i+5IkSYtSNwHs68CFEfGCci7XRcDjwGeB15Y+G4E7yvKO\n8pqy/jPZzfFPSZKkRWrec9CZeX9E3A48BBwGvkDr0OHfAOMR8Z7SdkP5khuA/xURU8AztK6YlCRp\nUejF/Qn3bL10wd9Ti0NXJwFk5nXAdTOanwLOn6XvD4DXdbM9SZKkpaCZZ2Gqb7zDvSRJPgtSkiSp\nOgOYJElSZQYwSZKkygxgkiRJlRnAJEmSKjOASZIkVWYAkyRJqswAJkmSVJkBTJIkqTIDmCRJUmUG\nMEmSpMoMYJIkSZX5MG4tCB+yLUlS55wBkyRJqswAJkmSVJmHICVJGnCdnuaxZ+ulPR6JFoozYJIk\nSZUZwCRJkirzEKQkSX3iFeTN5QyYJElSZV0FsIhYHhG3R8SXIuKJiPiViDgjIu6JiCfL59NL34iI\nD0XEVEQ8EhHnLkwJkiRJi0u3M2AfBP5vZv4S8MvAE8C1wL2ZuRa4t7wGuBhYWz42Ax/tctuSJEmL\n0rwDWEQsA34duAEgM3+Umd8BNgA3lW43AZeV5Q3AzdmyC1geEWfOe+SSJEmLVDczYGcB3wL+Z0R8\nISI+FhGnAcOZua/0+SYwXJZXAt9o+/q9pU2SJKlRIjPn94URI8Au4OWZeX9EfBD4LvDWzFze1u/Z\nzDw9Iu4Etmbm50r7vcA7MvPBGe+7mdYhSoaHh88bHx+f1/iOx6FDhxgaGur5dgbRQtU+OX1wAUZT\n3/CpsP+5fo+iP6y936PonybXv9RrX7dy2THX+/9db2sfGxvbnZkjnfTt5jYUe4G9mXl/eX07rfO9\n9kfEmZm5rxxiPFDWTwOr275+VWn7KZm5DdgGMDIykqOjo10MsTMTExPU2M4gWqjaNy3SS6m3rDvM\n9ZPNvBuLtTezdmh2/Uu+9snvHXP1lnU/5vrPfa+Rd8wftP/r530IMjO/CXwjIl5ami4CHgd2ABtL\n20bgjrK8A3hjuRryQuBg26FKSZKkxuj2z4C3ArdExEnAU8CbaIW62yLiKuBrwOtL353AJcAU8P3S\nV5IkqXG6CmCZ+TAw27HOi2bpm8DV3WxPkiRpKfBO+JIkSZUZwCRJkipbwpeC6FiOPAB2y7rDi/YK\nRkmSFitnwCRJkiozgEmSJFVmAJMkSarMACZJklSZAUySJKkyA5gkSVJlBjBJkqTKvA/YErPGe3pJ\nkjTwnAGTJEmqzBkwSZIaptOjJXu2XtrjkTSXM2CSJEmVGcAkSZIqM4BJkiRVZgCTJEmqzAAmSZJU\nmVdBLgLe20uSpKXFACZJkmZ1PBMA3rLi+HgIUpIkqTIDmCRJUmVdB7CIOCEivhARd5bXZ0XE/REx\nFREfj4iTSvvJ5fVUWb+m221LkiQtRgsxA/Y24Im21+8DPpCZvwg8C1xV2q8Cni3tHyj9JEmSGqer\nABYRq4BLgY+V1wG8Ari9dLkJuKwsbyivKesvKv0lSZIaJTJz/l8ccTvwR8ALgd8DNgG7yiwXEbEa\nuCszz4mIR4H1mbm3rPsKcEFmPj3jPTcDmwGGh4fPGx8fn/f4OnXo0CGGhoZ6vp2ZJqcPVt/mTMOn\nwv7n+j2K/mly/dbe71H0T5Prb3Lt0Nv6161c1ps3XiA1/q8fGxvbnZkjnfSd920oIuLVwIHM3B0R\no/N9n5kycxuwDWBkZCRHRxfsrY9qYmKCGtuZadMA3N9ry7rDXD/Z3LuRNLl+a29m7dDs+ptcO/S2\n/j1XjvbkfRdKv/6vP5pu9sLLgddExCXAKcA/BT4ILI+IEzPzMLAKmC79p4HVwN6IOBFYBny7i+1L\nkiQtSvM+Bywz35mZqzJzDXA58JnMvBL4LPDa0m0jcEdZ3lFeU9Z/Jrs5/ilJkrRI9eI+YO8Afjci\npoAXATeU9huAF5X23wWu7cG2JUmSBt6CHAjOzAlgoiw/BZw/S58fAK9biO1JkiQtZs09E1GSJC2Y\nTp8b6TMjWwxgkiSpGoNai8+ClCRJqswAJkmSVJkBTJIkqTIDmCRJUmWehN8DnZ5gKEmSmskZMEmS\npMoMYJIkSZUZwCRJkiozgEmSJFXmSfjHwZPrJUmqY6nfMd8ZMEmSpMoMYJIkSZUZwCRJkiozgEmS\nJFVmAJMkSarMACZJklSZAUySJKkyA5gkSVJlBjBJkqTK5n0n/IhYDdwMDAMJbMvMD0bEGcDHgTXA\nHuD1mflsRATwQeAS4PvApsx8qLvhS5KkJuv0jvk3rj+txyM5Pt08iugwsCUzH4qIFwK7I+IeYBNw\nb2ZujYhrgWuBdwAXA2vLxwXAR8vnvpucPsgmHzMkSZIqmfchyMzcd2QGKzP/DngCWAlsAG4q3W4C\nLivLG4Cbs2UXsDwizpz3yCVJkhapyMzu3yRiDXAfcA7w9cxcXtoDeDYzl0fEncDWzPxcWXcv8I7M\nfHDGe20GNgMMDw+fNz4+3vX45nLgmYPsf67nmxlIw6fS2Nqh2fVbe79H0T9Nrr/JtUOz6z9r2QkM\nDQ31dBtjY2O7M3Okk77dHIIEICKGgL8Cficzv9vKXC2ZmRFxXAkvM7cB2wBGRkZydHS02yHO6c9u\nuYPrJ7v+p1iUtqw73Njaodn1W3sza4dm19/k2qHZ9d+4/jRqZIpOdXUVZET8DK3wdUtmfqI07z9y\naLF8PlDap4HVbV++qrRJkiQ1yrwDWDm8eAPwRGa+v23VDmBjWd4I3NHW/sZouRA4mJn75rt9SZKk\nxaqbeciXA28AJiPi4dL2+8BW4LaIuAr4GvD6sm4nrVtQTNG6DcWbuti2JEnSojXvAFZOpo+jrL5o\nlv4JXD3f7UmSJC0V3glfkiSpMgOYJElSZQYwSZKkygxgkiRJlRnAJEmSKjOASZIkVWYAkyRJqswA\nJkmSVJkBTJIkqTIDmCRJUmUGMEmSpMoMYJIkSZUZwCRJkiozgEmSJFVmAJMkSarMACZJklSZAUyS\nJKkyA5gkSVJlBjBJkqTKDGCSJEmVGcAkSZIqqx7AImJ9RHw5IqYi4tra25ckSeq3qgEsIk4APgJc\nDJwNXBERZ9ccgyRJUr/VngE7H5jKzKcy80fAOLCh8hgkSZL6qnYAWwl8o+313tImSZLUGJGZ9TYW\n8VpgfWa+ubx+A3BBZl7T1mczsLm8fCnw5QpDWwE8XWE7g6jJtUOz67f25mpy/U2uHZpdf43afyEz\nf7aTjif2eCAzTQOr216vKm0/kZnbgG01BxURD2bmSM1tDoom1w7Nrt/am1k7NLv+JtcOza5/0Gqv\nfQjyAWBtRJwVEScBlwM7Ko9BkiSpr6rOgGXm4Yi4BrgbOAHYnpmP1RyDJElSv9U+BElm7gR21t7u\nHKoe8hwwTa4dml2/tTdXk+tvcu3Q7PoHqvaqJ+FLkiTJRxFJkiRVZwCTJEmqrJEBLCL+JCK+FBGP\nRMQnI2L5UfotuedWRsTrIuKxiPiHiDjq5bgRsSciJiPi4Yh4sOYYe+k46l+K+/6MiLgnIp4sn08/\nSr8fl/3+cEQs6quU59qPEXFyRHy8rL8/ItbUH2XvdFD/poj4Vtv+fnM/xrnQImJ7RByIiEePsj4i\n4kPl3+WRiDi39hh7qYP6RyPiYNt+/6+1x9grEbE6Ij4bEY+X3/Vvm6XPYOz/zGzcB/BK4MSy/D7g\nfbP0OQH4CvAS4CTgi8DZ/R77AtT+L2jd4HYCGDlGvz3Ain6Ptx/1L+F9/8fAtWX52tm+78u6Q/0e\n6wLVO+d+BH4b+O9l+XLg4/0ed+X6NwEf7vdYe1D7rwPnAo8eZf0lwF1AABcC9/d7zJXrHwXu7Pc4\ne1T7mcC5ZfmFwN/O8n0/EPu/kTNgmfmpzDxcXu6idUPYmZbkcysz84nMrPF0gYHUYf1Lct/TquGm\nsnwTcFkfx1JDJ/ux/d/kduCiiIiKY+ylpfp9PKfMvA945hhdNgA3Z8suYHlEnFlndL3XQf1LVmbu\ny8yHyvLfAU/w/EceDsT+b2QAm+G3aCXhmZr+3MoEPhURu8vjoZpkqe774czcV5a/CQwfpd8pEfFg\nROyKiMUc0jrZjz/pU/4oOwi8qMroeq/T7+N/Vw7D3B4Rq2dZvxQt1Z/x4/ErEfHFiLgrIl7W78H0\nQjml4F8B989YNRD7v/p9wGqJiE8DPzfLqndl5h2lz7uAw8AtNcfWa53U3oFfy8zpiHgxcE9EfKn8\nVTXwFqj+RelYtbe/yMyMiKPdg+YXyr5/CfCZiJjMzK8s9Fg1EP4PcGtm/jAi/iOt2cBX9HlM6r2H\naP2cH4qIS4C/Btb2eUwLKiKGgL8Cficzv9vv8cxmyQawzPyNY62PiE3Aq4GLshwUnmHO51YOqrlq\n7/A9psvnAxHxSVqHMxZFAFuA+pfkvo+I/RFxZmbuK9PtB47yHkf2/VMRMUHrL8jFGMA62Y9H+uyN\niBOBZcC36wyv5zp59m57rR+jdZ5gEyzan/GF0B5IMnNnRPx5RKzIzCXxkO6I+Bla4euWzPzELF0G\nYv838hBkRKwH3g68JjO/f5RujX1uZUScFhEvPLJM66KFWa+mWaKW6r7fAWwsyxuB580GRsTpEXFy\nWV4BvBx4vNoIF1Yn+7H93+S1wGeO8gfZYjRn/TPOe3kNrfNlmmAH8MZyNdyFwMG2w/NLXkT83JFz\nHSPifFpZYEn84VHqugF4IjPff5Rug7H/+3Hmf78/gClax38fLh9HroL6eWBnW79LaF1B8RVah6/6\nPvYFqP3f0Dre/UNgP3D3zNppXTX1xfLx2FKpvdP6l/C+fxFwL/Ak8GngjNI+AnysLP8qMFn2/SRw\nVb/H3WXNz9uPwLtp/fEFcArwl+V3wueBl/R7zJXr/6PyM/5F4LPAL/V7zAtU963APuDvy8/7VcBb\ngLeU9QF8pPy7THKMK8IX40cH9V/Ttt93Ab/a7zEvYO2/Rusc5kfa/o+/ZBD3v48ikiRJqqyRhyAl\nSZL6yQAmSZJUmQFMkiSpMgOYJElSZQYwSZKkygxgkiRJlRnAJEmSKvv/nvkkQauRUH0AAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6c32791f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets user Pandas DataFrame to see how this looks!\n",
    "sess = tf.Session()\n",
    "b_pd = pd.DataFrame(sess.run(b))\n",
    "sess.close() # Remember to always close the session!\n",
    "b_pd.iloc[1, :].hist(bins=50, figsize=(10, 5))\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Note:*** That's a beautifull normal distribution!  :')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Continuation of Prerequisites*\n",
    "\n",
    "* **Softmax:** is a function that converts K-dimensional vector ‘x’ containing real values to the same shaped vector of real values in the range of (0,1), whose sum is 1. We shall apply the softmax function to the output of our convolutional neural network in order to, convert the output to the probability for each class.\n",
    "\n",
    "$$ o(x)_{j} = \\frac{ e^{x_{i}} }{\\ \\sum_{n=1}^N e^{x_{i}}\\ } \n",
    "                \\bigg \\} \\ j = 1 \\dotsc N  $$\n",
    "\n",
    "> This is a type of *normalization*. This one in particular is also called [*normalized exponential function*](https://en.wikipedia.org/wiki/Softmax_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading inputs\n",
    "\n",
    "I have used 2000 images of dogs and cats each from [Kaggle dataset](https://www.kaggle.com/c/dogs-vs-cats) but you could use any n image folders on your computer which contain different kinds of objects. Typically, we divide our input data into 3 parts:\n",
    "\n",
    "1. ***Training data:*** we shall use 80% i.e. 0 images for training. <br /><br />\n",
    "\n",
    "1. ***Validation data:*** 20% images will be used for validation. These images are taken out of training data to calculate accuracy independently during the training process. <br /><br />\n",
    "\n",
    "1. ***Test set:*** separate independent data for testing which has around 400 images. Sometimes due to something called Overfitting; after training, neural networks start working very well on the training data(and very similar images) i.e. the cost becomes very small, but they fail to work well for other images. For example, if you are training a classifier between dogs and cats and you get training data from someone who takes all images with white backgrounds. It’s possible that your network works very well on this validation data-set, but if you try to run it on an image with a cluttered background, it will most likely fail. So, that’s why we try to get our test-set from an independent source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to read training images\n",
      "Now going to read dogs files (Index: 0)\n",
      "Now going to read cats files (Index: 1)\n",
      "<dataset.read_train_sets.<locals>.DataSets object at 0x7f6ca8e82048>\n"
     ]
    }
   ],
   "source": [
    "classes = ['dogs', 'cats']\n",
    "num_classes = len(classes)\n",
    " \n",
    "train_path='training_data'\n",
    "\n",
    "# validation split\n",
    "validation_size = 0.2\n",
    "\n",
    "# 128 x 128 RGB image\n",
    "img_size = 128\n",
    "\n",
    "# RGB images have 3 channels\n",
    "num_channels = 3\n",
    "\n",
    "# batch size\n",
    "batch_size = 16\n",
    " \n",
    "data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> dataset is a class that the author created to facilitate reading the input data. This is a simple python code that reads images from the provided training and testing data folders.\n",
    "\n",
    "The objective of our training is to learn the correct values of weights/biases for all the neurons in the network that work to do classification between dog and cat. **The Initial value of these weights be anything, but it works better if you take normal distributions (with mean zero and small variance)**. There are other methods to initialize the network but normal distribution is more prevalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s define functions to create the initial weights quickly just by specifying the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def create_biases(size):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating network layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building convolution layer in TensorFlow\n",
    "\n",
    "**`tf.nn.conv2d`** function can be used to build a convolutional layer which takes these inputs:\n",
    "\n",
    "* **input** = the output(activation) from the previous layer. This should be a 4-D tensor. Typically, in the first convolutional layer, you pass n images of size **`width * height * num_channels`**, then this has the size `[n width height num_channels]` <br /> <br />\n",
    "\n",
    "* **filter** = trainable variables defining the filter. We start with a random normal distribution and learn these weights. It’s a 4D tensor whose specific shape is predefined as part of network design. If your filter is of size `filter_size` and input fed has `num_input_channels` and you have `num_filters` filters in your current layer, then filter will have following shape: \n",
    "\n",
    "    * `[filter_size, filter_size, num_input_channels, num_filters]`<br /> <br />\n",
    "\n",
    "* **strides** = defines how much you move your filter when doing convolution. In this function, it needs to be a Tensor of **`size>=4`** i.e. **`[batch_stride x_stride y_stride depth_stride]`**. batch_stride is always 1 as you don’t want to skip images in your batch. `x_stride` and `y_stride` are same mostly and the choice is part of network design and we shall use them as 1 in our example. `depth_stride` is always set as 1 as you don’t skip along the depth. <br /> <br />\n",
    "\n",
    "* **padding** = SAME means we shall 0 pad the input such a way that output `x,y` dimensions are same as that of input. <br /> <br />\n",
    "\n",
    "After convolution, we add the biases of that neuron, which are also learnable/trainable. Again we start with random normal distribution and learn these values during training.\n",
    "\n",
    "Now, we apply max-pooling using `tf.nn.max_pool` function that has a very similar signature as that of `conv2d` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convolutional_layer(input,\n",
    "                               num_input_channels,\n",
    "                               conv_filter_size,\n",
    "                               num_filters):\n",
    "    \n",
    "    ## We shall define the weights that will be trained using create_weights function.\n",
    "    weights = create_weights(shape=[conv_filter_size, conv_filter_size, num_input_channels, num_filters])\n",
    "    \n",
    "    ## We create biases using the create_biases function. These are also trained.\n",
    "    biases = create_biases(num_filters)\n",
    "\n",
    "    ## Creating the convolutional layer\n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=weights,\n",
    "                         strides=[1, 1, 1, 1],\n",
    "                         padding='SAME')\n",
    "\n",
    "    layer += biases\n",
    "\n",
    "    ## We shall be using max-pooling.  \n",
    "    layer = tf.nn.max_pool(value=layer,\n",
    "                            ksize=[1, 2, 2, 1],\n",
    "                            strides=[1, 2, 2, 1],\n",
    "                            padding='SAME')\n",
    "    ## Output of pooling is fed to Relu which is the activation function for us.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening layer\n",
    "\n",
    "The Output of a convolutional layer is a multi-dimensional Tensor. We want to convert this into a one-dimensional tensor. This is done in the Flattening layer. We simply use the reshape operation to create a single dimensional tensor as defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flatten_layer(layer):\n",
    "    layer_shape = layer.get_shape()\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    layer = tf.reshape(layer, [-1, num_features])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> <p>`num_elements()` just multiplies the values within the specified indexes.</p>\n",
    "\n",
    "> <p>Ex: <br /> `shape([ 2 5 10 ]).num_elements()` would return `(2 * 5 * 10)` where as <br /> `shape([ 2 5 10 ])[1:2].num_elements()` would return `( 2 * 5 )`</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully connected layer\n",
    "\n",
    "Now, let’s define a function to create a fully connected layer. Just like any other layer, we declare weights and biases as random normal distributions. In fully connected layer, we take all the inputs, do the standard z=wx+b operation on it. Also sometimes you would want to add a non-linearity(RELU) to it. So, let’s add a condition that allows the caller to add RELU to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fc_layer(input,          \n",
    "                    num_inputs,    \n",
    "                    num_outputs,\n",
    "                    use_relu=True):\n",
    "    \n",
    "    #Let's define trainable weights and biases.\n",
    "    weights = create_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = create_biases(num_outputs)\n",
    " \n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    \n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    " \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders and input\n",
    "\n",
    "Now, let’s create a placeholder that will hold the input training images. All the input images are read in dataset.py file and resized to 128 x 128 x 3 size. Input placeholder x is created in the shape of [None, 128, 128, 3]. The first dimension being None means you can pass any number of images to it. For this program, we shall pass images in the batch of 16 i.e. shape will be [16 128 128 3]. Similarly, we create a placeholder y_true for storing the predictions. For each image, we have two outputs i.e. probabilities for each class. Hence y_pred is of the shape [None 2] (for batch size 16 it will be [16 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images themself\n",
    "x = tf.placeholder(tf.float32, \n",
    "                   shape=[None, img_size,img_size, num_channels], \n",
    "                   name='x')\n",
    "\n",
    "# Real label for the image\n",
    "y_true = tf.placeholder(tf.float32, \n",
    "                        shape=[None, num_classes], \n",
    "                        name='y_true')\n",
    "\n",
    "# Predicted label for the image\n",
    "y_true_cls = tf.argmax(y_true, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network design\n",
    "\n",
    "We use the functions defined above to create various layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network graph params\n",
    "filter_size_conv1 = 3 \n",
    "num_filters_conv1 = 32\n",
    "\n",
    "filter_size_conv2 = 3\n",
    "num_filters_conv2 = 32\n",
    "\n",
    "filter_size_conv3 = 3\n",
    "num_filters_conv3 = 64\n",
    "    \n",
    "fc_layer_size = 128\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    # Define Network\n",
    "    layer_conv1 = create_convolutional_layer(input=x,\n",
    "                                             num_input_channels=num_channels,\n",
    "                                             conv_filter_size=filter_size_conv1,\n",
    "                                             num_filters=num_filters_conv1)\n",
    "\n",
    "    layer_conv2 = create_convolutional_layer(input=layer_conv1,\n",
    "                                             num_input_channels=num_filters_conv1,\n",
    "                                             conv_filter_size=filter_size_conv2,\n",
    "                                             num_filters=num_filters_conv2)\n",
    "\n",
    "    layer_conv3 = create_convolutional_layer(input=layer_conv2,\n",
    "                                            num_input_channels=num_filters_conv2,\n",
    "                                            conv_filter_size=filter_size_conv3,\n",
    "                                            num_filters=num_filters_conv3)\n",
    "\n",
    "    layer_flat = create_flatten_layer(layer_conv3)\n",
    "\n",
    "    # fc = Fully Connected\n",
    "    layer_fc1 = create_fc_layer(input=layer_flat,\n",
    "                                num_inputs=layer_flat.get_shape()[1:4].num_elements(),\n",
    "                                num_outputs=fc_layer_size,\n",
    "                                use_relu=True)\n",
    "\n",
    "    layer_fc2 = create_fc_layer(input=layer_fc1,\n",
    "                                num_inputs=fc_layer_size,\n",
    "                                num_outputs=num_classes,\n",
    "                                use_relu=False)\n",
    "\n",
    "    y_pred = tf.nn.softmax(layer_fc2, name=\"y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "As mentioned above, you can get the probability of each class by applying softmax to the output of fully connected layer.\n",
    "\n",
    "`y_pred = tf.nn.softmax(layer_fc2,name=\"y_pred\")`\n",
    "\n",
    "y_pred contains the predicted probability of each class for each input image. The class having higher probability is the prediction of the network. y_pred_cls = tf.argmax(y_pred, dimension=1)\n",
    "\n",
    "Now, let’s define the cost that will be minimized to reach the optimum value of weights. We will use a simple cost that will be calculated using a Tensorflow function softmax_cross_entropy_with_logits which takes the output of last fully connected layer and actual labels to calculate cross_entropy whose average will give us the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "Tensorflow implements most of the optimisation functions. We shall use AdamOptimizer for gradient calculation and weight optimization. We shall specify that we are trying to minimise cost with a learning rate of 0.0001.\n",
    "\n",
    "`optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)`\n",
    "\n",
    "As you know, if we run optimizer operation inside session.run(), in order to calculate the value of cost, the whole network will have to be run and we will pass the training images in a feed_dict(Does that make sense? Think about, what variable would you need to calculate cost and keep going up in the code). Training images are passed in a batch of 16(batch_size) in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can calculate the validation accuracy by passing accuracy in session.run() and providing validation images in a feed_dict.\n",
    "\n",
    "`val_acc = session.run(accuracy,feed_dict=feed_dict_validate)`\n",
    "\n",
    "Similarly, we also report the accuracy for the training images.\n",
    "\n",
    "`acc = session.run(accuracy, feed_dict=feed_dict_train)`\n",
    "\n",
    "As, training images along with labels are used for training, so in general training accuracy will be higher than validation. We report training accuracy to know that we are at least moving in the right direction and are at least improving accuracy in the training dataset. After each Epoch, we report the accuracy numbers and save the model using saver object in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iterations = 0\n",
    "saver = tf.train.Saver()\n",
    "# saver.save(session, 'dogs-cats-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(epoch, \n",
    "                  feed_dict_train, \n",
    "                  feed_dict_validate, \n",
    "                  val_loss,\n",
    "                  session):\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    val_acc = session.run(accuracy, feed_dict=feed_dict_validate)\n",
    "    msg = \"Training Epoch {0} --- Training Accuracy: {1:>6.1%}, Validation Accuracy: {2:>6.1%},  Validation Loss: {3:.3f}\"\n",
    "    print(msg.format(epoch + 1, acc, val_acc, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_iteration):\n",
    "    total_iterations = 0\n",
    "    loss = []\n",
    "    epochs = []\n",
    "    accuracy_array = []\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:   \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(total_iterations,\n",
    "                       total_iterations + num_iteration):\n",
    "\n",
    "            x_batch, y_true_batch, _, cls_batch = data.train.next_batch(batch_size)\n",
    "            x_valid_batch, y_valid_batch, _, valid_cls_batch = data.valid.next_batch(batch_size)\n",
    "            \n",
    "            feed_dict_tr = {\n",
    "                x: x_batch, \n",
    "                y_true: y_true_batch\n",
    "            }\n",
    "        \n",
    "            feed_dict_val = {\n",
    "                x: x_valid_batch,\n",
    "                y_true: y_valid_batch\n",
    "            }\n",
    "\n",
    "            session.run(optimizer, feed_dict=feed_dict_tr)\n",
    "\n",
    "            if i % int(data.train.num_examples/batch_size) == 0: \n",
    "                val_loss = session.run(cost, feed_dict=feed_dict_val)\n",
    "                epoch = int(i / int(data.train.num_examples/batch_size))    \n",
    "\n",
    "                loss.append(val_loss)\n",
    "                epochs.append(epoch)\n",
    "                accuracy_array.append(session.run(accuracy, feed_dict=feed_dict_tr))\n",
    "                \n",
    "                loss_map = {\n",
    "                    'loss': loss,\n",
    "                    'epochs': epochs,\n",
    "                    'accu': accuracy_array \n",
    "                }\n",
    "\n",
    "#                 if (epoch > 0) and (epoch % 5) == 0:\n",
    "#                     pd_loss = pd.DataFrame(loss_map)\n",
    "#                     pd_loss.plot(x=\"accu\", y=\"loss\", kind='line')\n",
    "#                     plt.show()  \n",
    "                \n",
    "                show_progress(epoch, \n",
    "                              feed_dict_tr, \n",
    "                              feed_dict_val,\n",
    "                              val_loss,\n",
    "                              session)\n",
    "\n",
    "                saver.save(session, './dogs-cats-model') \n",
    "                \n",
    "        total_iterations += num_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1 --- Training Accuracy:  50.0%, Validation Accuracy:  50.0%,  Validation Loss: 0.698\n",
      "Training Epoch 2 --- Training Accuracy:  50.0%, Validation Accuracy:  56.2%,  Validation Loss: 0.685\n",
      "Training Epoch 3 --- Training Accuracy:  50.0%, Validation Accuracy:  50.0%,  Validation Loss: 0.700\n",
      "Training Epoch 4 --- Training Accuracy:  56.2%, Validation Accuracy:  31.2%,  Validation Loss: 0.723\n",
      "Training Epoch 5 --- Training Accuracy:  62.5%, Validation Accuracy:  62.5%,  Validation Loss: 0.674\n",
      "Training Epoch 6 --- Training Accuracy:  68.8%, Validation Accuracy:  37.5%,  Validation Loss: 0.727\n",
      "Training Epoch 7 --- Training Accuracy:  68.8%, Validation Accuracy:  50.0%,  Validation Loss: 0.674\n",
      "Training Epoch 8 --- Training Accuracy:  62.5%, Validation Accuracy:  56.2%,  Validation Loss: 0.698\n",
      "Training Epoch 9 --- Training Accuracy:  68.8%, Validation Accuracy:  62.5%,  Validation Loss: 0.646\n",
      "Training Epoch 10 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.676\n",
      "Training Epoch 11 --- Training Accuracy:  81.2%, Validation Accuracy:  68.8%,  Validation Loss: 0.601\n",
      "Training Epoch 12 --- Training Accuracy:  75.0%, Validation Accuracy:  50.0%,  Validation Loss: 0.769\n",
      "Training Epoch 13 --- Training Accuracy:  68.8%, Validation Accuracy:  75.0%,  Validation Loss: 0.654\n",
      "Training Epoch 14 --- Training Accuracy:  68.8%, Validation Accuracy:  68.8%,  Validation Loss: 0.694\n",
      "Training Epoch 15 --- Training Accuracy:  75.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.530\n",
      "Training Epoch 16 --- Training Accuracy:  75.0%, Validation Accuracy:  68.8%,  Validation Loss: 0.625\n",
      "Training Epoch 17 --- Training Accuracy:  81.2%, Validation Accuracy:  68.8%,  Validation Loss: 0.545\n",
      "Training Epoch 18 --- Training Accuracy:  87.5%, Validation Accuracy:  62.5%,  Validation Loss: 0.812\n",
      "Training Epoch 19 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.714\n",
      "Training Epoch 20 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.696\n",
      "Training Epoch 21 --- Training Accuracy:  87.5%, Validation Accuracy:  68.8%,  Validation Loss: 0.503\n",
      "Training Epoch 22 --- Training Accuracy:  87.5%, Validation Accuracy:  75.0%,  Validation Loss: 0.699\n",
      "Training Epoch 23 --- Training Accuracy:  93.8%, Validation Accuracy:  81.2%,  Validation Loss: 0.524\n",
      "Training Epoch 24 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%,  Validation Loss: 0.828\n",
      "Training Epoch 25 --- Training Accuracy: 100.0%, Validation Accuracy:  68.8%,  Validation Loss: 0.891\n",
      "Training Epoch 26 --- Training Accuracy: 100.0%, Validation Accuracy:  68.8%,  Validation Loss: 0.713\n",
      "Training Epoch 27 --- Training Accuracy: 100.0%, Validation Accuracy:  68.8%,  Validation Loss: 0.467\n",
      "Training Epoch 28 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%,  Validation Loss: 0.797\n",
      "Training Epoch 29 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%,  Validation Loss: 0.650\n",
      "Training Epoch 30 --- Training Accuracy: 100.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.818\n",
      "Training Epoch 31 --- Training Accuracy: 100.0%, Validation Accuracy:  56.2%,  Validation Loss: 1.149\n",
      "Training Epoch 32 --- Training Accuracy: 100.0%, Validation Accuracy:  62.5%,  Validation Loss: 0.853\n",
      "Training Epoch 33 --- Training Accuracy: 100.0%, Validation Accuracy:  68.8%,  Validation Loss: 0.563\n",
      "Training Epoch 34 --- Training Accuracy: 100.0%, Validation Accuracy:  62.5%,  Validation Loss: 1.000\n",
      "Training Epoch 35 --- Training Accuracy: 100.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.749\n",
      "Training Epoch 36 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%,  Validation Loss: 1.181\n",
      "Training Epoch 37 --- Training Accuracy: 100.0%, Validation Accuracy:  62.5%,  Validation Loss: 1.381\n",
      "Training Epoch 38 --- Training Accuracy: 100.0%, Validation Accuracy:  43.8%,  Validation Loss: 1.348\n",
      "Training Epoch 39 --- Training Accuracy: 100.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.673\n",
      "Training Epoch 40 --- Training Accuracy:  93.8%, Validation Accuracy:  75.0%,  Validation Loss: 1.044\n",
      "Training Epoch 41 --- Training Accuracy: 100.0%, Validation Accuracy:  75.0%,  Validation Loss: 0.956\n",
      "Training Epoch 42 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%,  Validation Loss: 1.318\n",
      "Training Epoch 43 --- Training Accuracy: 100.0%, Validation Accuracy:  62.5%,  Validation Loss: 1.417\n",
      "Training Epoch 44 --- Training Accuracy: 100.0%, Validation Accuracy:  50.0%,  Validation Loss: 1.301\n",
      "Training Epoch 45 --- Training Accuracy: 100.0%, Validation Accuracy:  81.2%,  Validation Loss: 0.409\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch = 50 iterations or (num_of_images/batch)\n",
    "train(num_iteration=2250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
